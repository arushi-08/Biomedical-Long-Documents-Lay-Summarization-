{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c564fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "import os\n",
    "from accelerate import PartialState\n",
    "\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_device(\"cuda\")\n",
    "# else:\n",
    "#     torch.set_default_device(\"cpu\")\n",
    "\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "os.environ['HF_HOME'] = '/data_vault/hexai/huggingface/hub/'\n",
    "\n",
    "model_type = 't5-small' # orca13b\n",
    "model_id = \"google-t5/t5-small\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_quant_type=\"nf8\",\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'], cache_dir=os.environ['HF_HOME'], use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device_map=\"auto\", token=os.environ['HF_TOKEN'], cache_dir=os.environ['HF_HOME']\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(f\"nlp/model/{model_type}\", device_map=\"cuda:1\", torch_dtype=torch.float16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'], cache_dir=os.environ['HF_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7df812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = \"/data_vault/hexai/Biolaysum/biolaysumm2024_data/eLife_val.jsonl\"\n",
    "elife_train = pd.read_json(path_or_buf=data, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edc8bb4-fee7-484e-b7ed-c013b669da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text2text-generation\",\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    batch_size=4,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=600,\n",
    "    temperature = 0.3,\n",
    "    do_sample=True,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab94653a-db9f-4aa0-91ae-b41a0736c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b93376-d6d6-4d70-9212-3143fcc02842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, LLMChain, ReduceDocumentsChain, StuffDocumentsChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6d6597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "\n",
    "    metadata[\"lay_summary\"] = record.get(\"lay_summary\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b21306-f0e6-4a41-a998-2d391d61d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json():\n",
    "    # Load the pdf file\n",
    "    loader = JSONLoader(\n",
    "        file_path=\"/data_vault/hexai/Biolaysum/biolaysumm2024_data/eLife_val.jsonl\",\n",
    "        jq_schema='.',\n",
    "        content_key=\"article\",\n",
    "        metadata_func=metadata_func,\n",
    "        json_lines=True\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    token_count = num_tokens_from_string(str(documents), \"cl100k_base\")\n",
    "    print(f'JSON Token Count: {token_count}')\n",
    "    return documents, token_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9265bc25-2491-4399-b75d-a9160997c7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Token Count: 3475695\n"
     ]
    }
   ],
   "source": [
    "docs, counts = load_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df54aac4-d40b-4c5e-abad-dae7223aba22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.251953125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19585/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3da35c41-9d07-4993-bbc4-79e7243e6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "00babcbd-3598-4ba8-afeb-6c2879536397",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.create_documents([docs[40].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9e3914c5-3696-48be-b002-992e335cb7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a854ee70-4250-4f16-b7f7-e09e232f7f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "t5_enc_model = T5EncoderModel.from_pretrained(\"google-t5/t5-small\").to(\"cuda\")\n",
    "extractor = pipeline(\n",
    "    model=t5_enc_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"feature-extraction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "56f0a4fb-bd46-4054-be23-a611e9a5ff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1515 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "embedd = extractor(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1cf3116-9823-4c1d-ad7b-1ecb52d9d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dedc7453-b46c-4394-9815-0b475b2ccf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for doc in splits:\n",
    "    embedd = extractor(doc.page_content)\n",
    "    mean_embedd = np.array(embedd).mean(axis=1).squeeze(axis=0)\n",
    "    document_embeddings.append(mean_embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3b0f374a-24d8-4589-b39f-1643cbe864cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nump_embedd = np.array(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3dab88a7-ac41-4eaa-9e2a-972af48d2c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 512)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nump_embedd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3a35164-8c64-4467-b290-f21cdb865bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'embeddings' is a list or array of 1536-dimensional embeddings\n",
    "\n",
    "# Choose the number of clusters, this can be adjusted based on the book's content.\n",
    "# I played around and found ~10 was the best.\n",
    "# Usually if you have 10 passages from a book you can tell what it's about\n",
    "num_clusters = 5\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(nump_embedd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eb6566e3-f1da-4500-a109-111e39e3bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest embeddings to the centroids\n",
    "\n",
    "# Create an empty list that will hold your closest points\n",
    "closest_indices = []\n",
    "\n",
    "# Loop through the number of clusters you have\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    # Get the list of distances from that particular cluster center\n",
    "    distances = np.linalg.norm(nump_embedd - kmeans.cluster_centers_[i], axis=1)\n",
    "    \n",
    "    # Find the list position of the closest one (using argmin to find the smallest distance)\n",
    "    closest_index = np.argmin(distances)\n",
    "    \n",
    "    # Append that position to your closest indices list\n",
    "    closest_indices.append(closest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "810f96a9-4730-42ad-b14d-993174809312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 6]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_indices = sorted(closest_indices)\n",
    "selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c87c6a11-5a1e-4848-b825-710b2eed8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt  = \"\"\" Summarize:\n",
    "```{text}```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1ade3164-b4a9-4912-a62b-8313e870c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8d1e67c3-fcb8-4c42-b8af-6ea10955c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_summarize_chain\n",
    "map_chain = load_summarize_chain(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 \n",
    "                                 prompt=map_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a290b20e-5ee1-4090-a323-612aa8b67d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_docs = [splits[doc] for doc in selected_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23f542-c752-4491-966f-d71677f7db1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary #0 (chunk #0) - Preview: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "Summary #1 (chunk #1) - Preview: compared to the control monkeys . a neurologist and two neuroscientists compared the results to those obtained . monkey S performed a brain scan of the lGN . a scan of the lGN showed an almost complete loss of primary visual cortex . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make an empty list to hold your summaries\n",
    "summary_list = []\n",
    "\n",
    "# Loop through a range of the lenght of your selected docs\n",
    "for i, doc in enumerate(selected_docs):\n",
    "    \n",
    "    # Go get a summary of the chunk\n",
    "    chunk_summary = map_chain.run([doc])\n",
    "    \n",
    "    # Append that summary to your list\n",
    "    summary_list.append(chunk_summary)\n",
    "    \n",
    "    print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[:250]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df8423ae-049d-4421-82f5-e016c319154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your total summary has 232 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "summaries = \"\\n\".join(summary_list)\n",
    "\n",
    "# Convert it back to a document\n",
    "summaries = Document(page_content=summaries)\n",
    "\n",
    "print (f\"Your total summary has {llm.get_num_tokens(summaries.page_content)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ff3c21dd-09e5-424f-86b2-13fb649a2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_prompt = \"\"\"\n",
    "Summarize:\n",
    "\"\"\"\n",
    "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e4ae9bbb-da44-47fb-8e44-e7a94ec74678",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_chain = load_summarize_chain(llm=llm,\n",
    "                             chain_type=\"stuff\",\n",
    "                             prompt=combine_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "350ef490-4a63-4aff-bd7d-1863d1ba7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = reduce_chain.run([summaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f1291879-c030-4964-bcb8-87c959974d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of summaries from a medical article. The summaries will be enclosed in triple backticks () Your goal is to give a verbose summary of what happened in the book . 'a single passage of a medical article. a single passage of a medical article. a single passage of a medical article. a single passage of a medical article. a single passage of a medical article. a single passage of a medical article. a\n"
     ]
    }
   ],
   "source": [
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe4439-97e0-4bc4-844e-10ddcd553cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
